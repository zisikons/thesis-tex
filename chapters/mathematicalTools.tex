\chapter{Μαθηματικά Εργαλεία}

Στο κεφάλαιο αυτό γίνεται εισαγωγή των μαθηματικών εργαλείων και βασικών εννοιών στα οποία βασίζεται η διπλωματική εργασία. Σκοπός του κεφαλαίου είναι η συμφιλίωση του αναγνώστη με αυτές τις έννοιες, έτσι ώστε να γίνει ομαλή η μετάβαση στα αποτελέσματα της διπλωματικής εργασίας στα κεφάλαια που ακολουθούν. 

Συνοπτικά, τρία είναι τα εργαλεία που θα παρουσιαστούν. Αρχικά θα γίνει μια εισαγωγή στα νευρωνικά δίκτυα RBF με έμφαση στην αρχιτεκτονική τους, στον τρόπο που χρησιμοποιούνται στις εφαρμογές αναγνώρισης και ελέγχου, καθώς και τις προσεγγιστικές τους ιδιότητες. Στην συνέχεια θα παρουσιαστεί ο Έλεγχος Προδιαγεγραμμένης Απόκρισης, ο σκοπός που χρησιμοποιείται σε αυτή την εργασία καθώς και επιχειρήματα που αποδεικνύουν την εγκυρότητα του, τόσο σε μαθηματικό επίπεδο όσο και με την χρήση προσομοιώσεων. Τέλος, θα γίνει μια συνοπτική εισαγωγή στην αρχιτεκτονική του υποσυστήματος που παράγει τα σήματα αναφοράς του σχήματος, καθώς και ποιοι είναι οι λόγοι που χρησιμοποιείται αυτή η μεθοδολογία και όχι κάποια άλλη.


%Artificial neural networks (ANNs) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains.[1] Such systems "learn" (i.e. progressively improve performance on) tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the results to identify cats in other images. They do this without any a priori knowledge about cats, e.g., that they have fur, tails, whiskers and cat-like faces. Instead, they evolve their own set of relevant characteristics from the learning material that they process.

\section{Νευρωνικά Δίκτυα RBF}
Με τον όρο \textit{Tεχνητά Νευρωνικά Δίκτυα} (Artificial Neural Networks) αναφερόμαστε σε μια κατηγορία μαθηματικών μοντέλων τα οποία προέκυψαν την δεκαετία του 1940 και είναι εμπνευσμένα από τα βιολογικά νευρωνικά δίκτυα τα οποία απαντώνται στους εγκεφάλους των ανθρώπων και των ζώων. Το κύριο χαρακτηριστικό των νευρωνικών δικτύων είναι η εγγενής ικανότητα μάθησης. Ως μάθηση μπορεί να οριστεί η σταδιακή βελτίωση της ικανότητας του δικτύου να επιλύει κάποιο πρόβλημα (π.χ. η σταδιακή προσέγγιση μίας συνάρτησης). Η μάθηση επιτυγχάνεται μέσω της εκπαίδευσης, μίας επαναληπτικής διαδικασίας σταδιακής προσαρμογής των παραμέτρων του δικτύου σε τιμές κατάλληλες ώστε να επιλύεται με επαρκή επιτυχία το προς εξέταση πρόβλημα.


Υπάρχουν πολλές κατηγορίες νευρωνικών δικτύων όπως τα Συνελικτικά Νευρωνικά Δίκτυα (Convolutional Neural Networks), τα Αναδρομικά Νευρωνικά Δίκτυα (Recurrent Neural Networks), τα Ακτινικά Νευρωνικά Δίκτυα (Radial Basis Networks), τα Πιθανοτικά Νευρωνικά Δίκτυα (Probabilistic Neural Networks) και άλλα, κάθε ένα κατάλληλο για διαφορετικό τύπο εφαρμογών. Εμείς θα χρησιμοποιήσουμε τα ακτινικά νευρωνικά δίκτυα τα οποία από εδώ και στο εξής θα αναφέρουμε ως \textit{δίκτυα RBF} καθώς είναι τα καταλληλότερα για προσέγγιση συναρτήσεων.

\subsection{Αρχιτεκτονική}
Η αρχιτεκτονική ενός RBF νευρωνικού δικτύου παρουσιάζεται στο σχήμα $(\ref{fig:rbf_network_architecture})$. Ένα RBF νευρωνικό δίκτυο αποτελείται από τρία επίπεδα, το επίπεδο εισόδου, το ενδιάμεσο ή κρυφό επίπεδο, και το επίπεδο εξόδου.

\begin{figure}
	\centering
	\input{./images/tikz/rbf_network.tex}
	\caption{ Αρχιτεκτονική Νευρωνικού Δικτύου RBF }
	\label{fig:rbf_network_architecture}
\end{figure}

\textbf{Επίπεδο Εισόδου}: Η είσοδος ενός δικτύου RBF είναι ένα διάνυσμα $x = \begin{bmatrix} x_1,x_2, ..., x_n \end{bmatrix}^T$. Στα νευρωνικά δίκτυα αυτής της κατηγορίας το επίπεδο εισόδου είναι υπεύθυνο για την μετάδοση των επιμέρους ορισμάτων $x_i$ σε κάθε νευρώνα και όχι για κάποια περαιτέρω προεπεξεργασία δεδομένων.

\textbf{Κρυφό Επίπεδο:} Στο κρυφό επίπεδο λαμβάνει χώρο ο υπολογισμός των συναρτήσεων $\varphi_i(x)$ οι οποίες ονομάζονται \textit{συναρτήσεις βάσης} ή \textit{συναρτήσεις ενεργοποίησης}. Σε άλλους τύπους δικτύων μπορεί να υπάρχουν πάνω από ένα κρυφά επίπεδα, αλλά στα RBF δίκτυα που μελετάμε υπάρχει μόνο ένα.

Κάθε συνάρτηση βάσης είναι μια πραγματική συνάρτηση $\varphi: \mathbb{R}^n \rightarrow \mathbb{R} $, και αντιστοιχεί σε ένα σημείο του χώρου $c \in \mathbb{R}^n$. Οι συναρτήσεις $\varphi$ είναι ακτινικές συναρτήσεις, που σημαίνει πως το αποτέλεσμα τους εξαρτάται από την απόσταση της εισόδου $x$ από το σημείο $c$, ή αλλιώς μαθηματικά:
\begin{equation*}
	\varphi(x,c) = \varphi(\| x - c \|)
\end{equation*}
και από εκεί προκύπτει και η ονομασία Ακτινικά Νευρωνικά Δίκτυα. Τυπικές ακτινικές συναρτήσεις είναι η γκαουσιανή, η τετραγωνική, η αντίστροφη τετραγωνική και άλλες~\cite{wiki:rbf}, ωστόσο σε αυτήν την εργασία θα χρησιμοποιήσουμε τις γκαουσιανές συναρτήσεις οι οποίες έχουν την μορφή
\begin{equation*}
	\varphi(x) = \exp \left( - \left\| \frac{x - c}{\sigma}\right\|^2 \right)
\end{equation*}
Το σημείο $c$ ονομάζεται \textit{κέντρο} ενώ η ποσότητα $\sigma$ ονομάζεται \textit{διασπορά}. Στην εφαρμογή μας, και τα δυο αυτά μεγέθη επιλέγονται εκ των προτέρων και παραμένουν σταθερά κατά την διάρκεια των πειραμάτων. Ένα παράδειγμα μιας μονοδιάστασης γκαουσιανής συνάρτησης φαίνεται στο σχήμα $(\ref{fig:gaussian_function})$.


\textbf{Επίπεδο Εξόδου:} Η έξοδος ενός RBF νευρωνικού δικτύου είναι μια βαθμωτή συνάρτηση $f: \mathbb{R}^n \rightarrow \mathbb{R} $. Η έξοδος αυτή υπολογίζεται στο επίπεδο εξόδου ώς το σταθμισμένο άθροισμα:
\begin{equation*}
	f(x) = \sum_{i = 1}^{N} w_i \varphi_i (x)
\end{equation*}
Τα βάρη $w_i$ ονομάζονται \textit{συναπτικά βάρη}, και αποτελούν τις ελεύθερες παραμέτρους του δικτύου. Καθώς οι συναρτήσεις βάσης $\varphi_i(x)$ είναι προεπιλεγμένες και σταθερές, λέμε ότι το μοντέλο είναι \textit{γραμμικά παραμετροποιημένο} που θα πει ότι η έξοδος του είναι γραμμικός συνδυασμός των ελεύθερων παραμέτρων αυτού. Τέλος, η έξοδος του μοντέλου μπορεί να γραφτεί και διανυσματικά ώς:
\begin{equation*}
	f(x) = \begin{bmatrix} w_1 & w_2 & ... & w_n \end{bmatrix} \cdot \begin{bmatrix}
	\varphi_1(x) \\ \varphi_2(x) \\ ... \\ \varphi_n(x)
	\end{bmatrix} = 
	W^T \cdot  \varPhi(x)
\end{equation*}
Το διάνυσμα $W$ ονομάζεται \textit{διάνυσμα βαρών} ενώ το διάνυσμα $\varPhi(x)$ ονομάζεται \textit{διάνυσμα οπισθοδρομητών} στην βιβλιογραφία της αναγνώρισης συστημάτων. Η γραμμικότητα ως προς τις ελεύθερες παραμέτρους αποτελεί σημαντικό πλεονέκτημα στα προβλήματα αναγνώρισης, για αυτό και τα γραμμικά παραμετροποίημενα μοντέλα προτιμούνται σε τέτοιου είδους εφαρμογές.

\begin{figure}
	\centering
	\scalebox{.5}{\input{plots/mathematicalTools/networks/simple_gaussian.tex}}
	\caption{ Γκαουσιανή συνάρτηση ενεργοποίησης. Το κέντρο $c$ είναι το σημείο στο οποίο η συνάρτηση παρουσιάζει την μέγιστη τιμή, ενώ η διασπορά $\sigma$ καθορίζει τον ρυθμό που η συνάρτηση μειώνεται όσο το $x$ απομακρύνεται από το $c$. }
	\label{fig:gaussian_function}
\end{figure}


\begin{figure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[scale=0.5]{plots/mathematicalTools/networks/components.tex}
		\caption{Επιμέρους όροι}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[scale=0.5]{plots/mathematicalTools/networks/f_approximation.tex}
		\caption{Συνολική Προσέγγιση}
	\end{subfigure}
	\caption{ Προσέγγιση της $f(x) = 1+x$ από ένα απλό RBF νευρωνικό δίκτυο. }
	\label{fig:rbf_approximation}
\end{figure}

\subsection{Προσεγγιστικές Ιδιότητες}
Τα ακτινικά νευρωνικά δίκτυα είναι ένα πολύ χρήσιμο εργαλείο στην αναγνώριση μη γραμμικών συστημάτων. Ο λόγος είναι πως η παραπάνω δομή μπορεί να προσεγγίσει ικανοποιητικά καλά μια οποιαδήποτε συνεχή συνάρτηση όπως αναφέρεται στην ακόλουθη ιδιότητα~\cite{park1991universal}.

\textbf{Ιδιότητα Προσέγγισης:} \textit{Για κάθε συνεχή συνάρτηση $f: \mathbb{R}^n \rightarrow \mathbb{R} $ και κάθε θετική σταθερά $\epsilon > 0$ υπάρχουν ακέραιος αριθμός $q$, βέλτιστα βάρη $W^* \in \mathbb{R}^q$ και διανυσματικό πεδίο βάσης $\varPhi: \mathbb{R}^n \rightarrow \mathbb{R}^q$, τέτοια ώστε:
\begin{equation*}
	\max_{x \in \Omega_x} \left\| f(x) - W^{*T}\varPhi(x) \right\| \leq \epsilon
\end{equation*}
όπου $\Omega_x$ είναι ένα συμπαγές σύνολο προσέγγισης.
}

Σύμφωνα με την παραπάνω ιδιότητα αν το μέγεθος του διανυσματικού πεδίου βάσης $q$ είναι αρκούντως μεγάλο και οι συναρτήσεις που περιλαμβάνει είναι κατάλληλα επιλεγμένες τότε υπάρχουν βέλτιστα βάρη $W^*$ τέτοια ώστε η έξοδος του νευρωνικού δικτύου να προσεγγίζει οσοδήποτε καλά την άγνωστη συνάρτηση $f(x)$ μέσα στο σύνολο $\Omega_x$. Ως εκ τούτου, μπορούμε να αντικαταστήσουμε, χωρίς βλάβη γενικότητας, την άγνωστη συνάρτηση $f(x)$ με ένα RBF νευρωνικό δίκτυο ως εξής:
\begin{equation*}
	f(x) = W^{*T}\varPhi(x) + \epsilon_f(x)
\end{equation*}

Η ποσότητα $\epsilon_f(x)$ ονομάζεται \textit{σφάλμα μοντελοποίησης}, και εξαρτάται από την αρχιτεκτονική του νευρωνικού δικτύου, δηλαδή το πλήθος και την διάταξη των συναρτήσεων βάσης και αντίστοιχα των κέντρων και των διασπορών του νευρωνικού δικτύου.

\textbf{Παράδειγμα}\\
Στο παράδειγμα που ακολουθεί, θα γίνει μια επίδειξη των προσεγγιστικών ικανοτήτων ενός RBF νευρωνικού δικτύου. Για λόγους απλότητας θα προσεγγίσουμε την μονοδιάστατη συνάρτηση $f: \mathbb{R} \rightarrow \mathbb{R} $
\begin{equation*}
f(x) = 1+x
\end{equation*}
στο συμπαγές σύνολο $\Omega_x = [-1,1] \subset \mathbb{R}$. Για την προσέγγιση θα χρησιμοποιήσουμε ένα νευρωνικό δίκτυο RBF με τρία κέντρα κατανεμημένα στο $\Omega_x$. Τα κέντρα αυτά θα είναι τα $c = \begin{bmatrix} -0.7 & 0.0 & 0.7 \end{bmatrix}$ και η διασπορά $\sigma$ είναι $0.4$. 

{
\begin{wrapfigure}{r}{0.4\textwidth}
	\centering
	\includegraphics[scale=0.5]{plots/mathematicalTools/networks/f_approximation_dense.tex}
	\caption{Προσέγγιση με πυκνότερο δίκτυο}
	\label{fig:rbf_dense_approximation}	
\end{wrapfigure}

Τα αποτελέσματα της προσέγγισης φαίνονται στο σχήμα $(\ref{fig:rbf_approximation})$. Αρχικά στο σχήμα $(\ref{fig:rbf_approximation}.a)$ φαίνονται τα βέλτιστα βάρη για κάθε μια από τις συναρτήσεις βάσης, και το άθροισμα τους το οποίο αποτελεί την προσέγγιση $\hat{f}(x)$ φαίνεται στο σχήμα $(\ref{fig:rbf_approximation}.b)$. 

Όπως φαίνεται λοιπόν από το σχήμα $(\ref{fig:rbf_approximation}.b)$, το εκπαιδευμένο νευρωνικό δίκτυο καταφέρνει να προσεγγίσει ικανοποιητικά την άγνωστη συνάρτηση στην περιοχή ενδιαφέροντος $\Omega_x$. Το σφάλμα μοντελοποίησης $\epsilon_f(x)$ είναι η σκιαγραφημένη περιοχή, ενώ στο σχήμα φαίνεται και το μέγιστο σφάλμα $\epsilon$. Καθώς αυτό το σφάλμα είναι το ελάχιστο δυνατό για την δεδομένη αρχιτεκτονική που έχουμε επιλέξει, στην περίπτωση που απαιτείται μεγαλύτερη ακρίβεια, πρέπει να αυξήσουμε τον αριθμό των συναρτήσεων βάσης. Μια προσέγγιση με την χρήση ενός πυκνότερου δικτύου φαίνεται στο σχήμα $(\ref{fig:rbf_dense_approximation})$

}

Τέλος, είναι σημαντικό να σημειωθεί πως το θεώρημα μας εξασφαλίζει πως το σφάλμα $\epsilon_f(x)$ φράσσεται από το $\epsilon$ μόνο εντός του $\Omega_x$. Όπως φαίνεται και στο σχήμα $(\ref{fig:rbf_approximation}.b)$, όσο απομακρυνόμαστε από το $\Omega_x$ το σφάλμα μεγαλώνει ανεξέλεγκτα.


\section{Συνθήκη Επιμένουσας Διέγερσης}
Όπως είδαμε στο κεφάλαιο $2.1$, μπορούμε να χρησιμοποιήσουμε το μαθηματικό μοντέλο των νευρωνικών δικτύων $RBF$ για να προσεγγίσουμε αρκούντως καλά μια συνεχή μη-γραμμική συνάρτηση σε ένα πεδίο ενδιαφέροντος $\Omega_x$. Πώς όμως εξασφαλίζεται ότι τα βάρη $W$ θα συγκλίνουν στα βέλτιστα βάρη $W^*$ κατά την εκτίμηση παραμέτρων;

Η απάντηση στο παραπάνω ερώτημα δίνεται από την συνθήκη της επιμένουσας διέγερσης (Persistancy of Excitation Condition), η οποία δίνεται παρακάτω:

\textbf{Συνθήκη Επιμένουσας Διέγερσης:} 
\textit{Έστω $\mu$ ένα θετικό $\Sigma$-πεπερασμένο μέτρο Borel στο διάστημα $[0,\infty)$. Μια συνεχής, ομοιόμορφα φραγμένη διανυσματική συνάρτηση $\varPhi: [0,\infty) \rightarrow \mathbb{R}^q$ ικανοποιεί την συνθήκη Επιμένουσας Διέγερσης εάν υπάρχουν θετικές σταθερές $a_1$, $a_2$ και $T$ έτσι ώστε:
\begin{equation}
	a_1 \left\| W \right\|^2 
	\leq
	\int_{t_0}^{t_0 + T} \left| W^T \varPhi(\tau) \right| d\mu(\tau) 
	\leq
	a_2 \left\| W \right\|^2, \quad
	\forall \: t_0 \geq 0 , \quad
	\forall \: W \in \mathbb{R}^q
	\label{eq:pe_condition}
\end{equation}
}

Διαισθητικά το παραπάνω θεώρημα θα πει πως η συνθήκη επιμένουσας διέγερσης ικανοποιείται όταν η τροχιά του συστήματος εξερευνεί πλήρως τον χώρο ελεύθερων παραμέτρων. Η παρακάτω έννοια θα εξηγηθεί και στην συνέχεια με παραδείγματα.

\textbf{Παρατήρηση 1:} Στην παραπάνω εξίσωση, οι σταθερές $a_1$ και $a_2$ ονομάζονται επίπεδα διέγερσης, και οι τιμές τους είναι καθοριστικές για την απόδοση των αλγορίθμων αναγνώρισης (ρυθμός σύγκλισης, άνω φράγματα σφαλμάτων). Ωστόσο, το παραπάνω Θεώρημα είναι θεώρημα ύπαρξης αυτών των τιμών, και όχι υπολογισμού τους.

\textbf{Παρατήρηση 2:} Ο λόγος που το διάνυσμα οπισθοδρομητών $\varPhi(x)$ εμφανίζεται στην παραπάνω εξίσωση ως συνάρτηση του χρόνου $t$ είναι επειδή στην πράξη, το διάνυσμα καταστάσεων $x(t)$ είναι μια καμπύλη $x: [0,\infty) \rightarrow \mathbb{R}^n$. Η ικανοποίηση της ΣΕΔ στις εφαρμογές \textit{οnline} αναγνώρισης συστημάτων ανάγεται ακριβώς στο πρόβλημα της σχεδίασης ενός ελεγκτή που επιτυγχάνει την παρακολούθηση μιας τροχιάς η οποία διεγείρει επαρκώς την δυναμική συστήματος.

\textbf{Παρατήρηση 3:} Η ισχύς του παραπάνω θεωρήματος είναι γενική και δεν ισχύει μόνο για το μοντέλο των RBF νευρωνικών δικτύων. Εν αντιθέσει, το θεώρημα δεν θέτει κανέναν περιορισμό για την δομή του διανύσματος οπισθοδρομητών $varPhi(\tau)$, συνεπώς οποιαδήποτε επιλογή μοντέλου και σετ δεδομένων $x(t)$ μπορούν να χρησιμοποιηθούν.

\textbf{Παρατήρηση 4:} Είναι σημαντικό να τονίσουμε πως η ΣΕΔ το μόνο που εξασφαλίζει είναι ότι τα βέλτιστα βάρη για το επιλεγμένο μοντέλο μπορούν να βρεθούν. Αυτό δεν συνεπάγεται απαραίτητα ότι το μοντέλο αυτό είναι ικανό να προσεγγίσει επαρκώς την δυναμική του συστήματος (Παράδειγμα 2).

\subsection{Παράδειγμα 1: Μη ικανοποίηση της Σ.Ε.Δ}
Έστω ότι θέλουμε να προσεγγίσουμε την άγνωστη συνάρτηση $f(x) = \sqrt{x}$ στο κλειστό σύνολο $\Omega_x = [0,2]$. Για τον σκοπό αυτό, θα χρησιμοποιήσουμε δυο συναρτήσεις ράμπα τοποθετημένες στο $x=0$ και στο $x=1$. Καθώς η συνάρτηση ράμπα περιγράφεται από τον παρακάτω τύπο
\begin{equation*}
	R_i(x) = \begin{cases}
	x-i \:&, x \geq i\\
	0 \: &, x<i 
	\end{cases}
\end{equation*}
το μαθηματικό μοντέλο που χρησιμοποιούμε είναι το:
\begin{equation}
y = a R_0(x) + b R_1(x)
\label{eq:ramp_model}
\end{equation}
όπου $a$ και $b$ οι ελεύθεροι παράμετροι. Χρησιμοποιώντας αυτό το μοντέλο, η βέλτιστη δυνατή προσέγγιση φαίνεται στο σχήμα $(\ref{fig:sqrt_approximation})$.

\begin{figure}
	\centering
	\scalebox{1}{\input{plots/mathematicalTools/sqrt_approx.tex}}
	\caption{ Προσέγγιση της $\sqrt{x}$ από ράμπες}
	\label{fig:sqrt_approximation}
\end{figure}

Με την φράση \emph{"η τροχιά του συστήματος εξερευνεί πλήρως τον χώρο ελεύθερων παραμέτρων"} εννοούμε πως κατά την διάρκεια συλλογής δεδομένων, ανεξαρτήτως του αν ο αλγόριθμος είναι online ή offline, πρέπει η τροχιά $x(t)$ να διεγείρει όλες τις συνιστώσες του διανύσματος του διανύσματος οπισθοδρομητών.

Στο συγκεκριμένο παράδειγμα, το διάνυσμα οπισθοδρομητών είναι το
\begin{equation*}
	\varPhi(x(t)) = \begin{bmatrix}
	R_0(x(t)) \\ R_1(x(t)) 
	\end{bmatrix}
\end{equation*}
Εάν κατά την διάρκεια εκτέλεσης ενός πειράματος αναγνώρισης, η τροχιά $x(t)$ ανήκει εξολοκλήρου στο σύνολο $\Omega_1$ (σχήμα ...), τότε η εκτίμηση $\hat{f}(x)$ εκφυλίζεται σε:
\begin{equation*}
\begin{split}
\hat{f}(x) &=\begin{bmatrix} a & b \end{bmatrix} 
\begin{bmatrix} R_0(x) \\ R_1(x) \end{bmatrix} \xRightarrow{x \in \Omega_1}\\
&= a R_0(x)
\end{split}
\end{equation*}
και κατά συνέπεια η παράμετρος $b$ δεν επηρεάζει καθόλου το αποτέλεσμα. Συνεπώς, κανένας αλγόριθμος εκτίμησης παραμέτρων δεν μπορεί να εκτιμήσει σωστά αυτή την παράμετρο με αποτέλεσμα την αποτυχία του πειράματος αναγνώρισης.

Η παραπάνω συλλογιστική, μπορεί να αποδειχθεί και μαθηματικά χρησιμοποιώντας τον ορισμό της ΣΕΔ:
